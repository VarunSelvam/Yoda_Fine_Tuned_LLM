{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEwIIrIlXgYF",
        "outputId": "4bf2da34-aa5d-4412-b40d-082508b7b297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\varun\\Box Sync\\Python\\Python Projects\\Yoda_Agent\n",
            "['Additional_Scripts', 'archive (1)', 'dialogue_output.csv', 'local_llm.py', 'Official Yoda Prequel Dialouge', 'Official Yoda Prequel Dialouge - Context', 'script_agent.py', 'STAR WARS THE ATTACK OF THE CLONES.txt', 'STAR WARS THE PHANTOM MENACE.txt', 'STAR WARS THE REVENGE OF THE SITH.txt', 'star_wars_dialogue.csv', 'SW_EpisodeIV.txt', 'SW_EpisodeV.txt', 'SW_EpisodeVI.txt', 'TPM_Dialogue_Complete.xlsx', 'yoda_all.csv', 'YODA_AOTC - Copy.txt', 'YODA_AOTC.txt', 'yoda_chat.jsonl', 'yoda_chat_01.jsonl', 'yoda_chat_system.jsonl', 'yoda_data.ipynb', 'yoda_data_01.ipynb', 'yoda_dialogue.csv', 'yoda_model', 'Yoda_Model.ipynb', 'yoda_model_test.py', 'YODA_ROTS.txt', 'YODA_TPM - Copy.txt', 'YODA_TPM.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below code fixes the json format into a proper JSONL format. The output says yoda_chat_final, however I renamed it to yoda_chat_02 and I renamed it in windows explorer and in this code block as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to parse: {\"messages:\" [{\"role\": \"user\", \"content\": \"Is revenge a good thing\"}, {\"role\": \"assistant\", \"content...\n",
            "Error: Expecting ':' delimiter: line 1 column 14 (char 13)\n",
            "Processed 404 objects with Unicode decoding\n",
            "Entry 1: {'messages': [{'role': 'user', 'content': 'Impossible! The Sith have been extinct for a millennium.'}, {'role': 'assistant', 'content': 'The very Republic is threatened, if involved the Sith are.'}]}\n",
            "\n",
            "Entry 2: {'messages': [{'role': 'user', 'content': 'I do not believe they could have returned without us knowing.'}, {'role': 'assistant', 'content': 'Hard to see, the dark side is. Discover who this assassin is, we must.'}]}\n",
            "\n",
            "Entry 3: {'messages': [{'role': 'user', 'content': 'This attack was with purpose, that is clear, and I agree the Queen is the target.'}, {'role': 'assistant', 'content': 'With this Naboo queen you must stay, Qui-Gon. Protect her.'}]}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def decode_unicode_escapes(text):\n",
        "    \"\"\"Decode Unicode escape sequences in text\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Decode \\uXXXX sequences\n",
        "        return text.encode().decode('unicode_escape')\n",
        "    return text\n",
        "\n",
        "def fix_jsonl_with_unicode_decoding(input_file, output_file):\n",
        "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # First decode any Unicode escapes in the raw content\n",
        "    try:\n",
        "        content = content.encode().decode('unicode_escape')\n",
        "    except:\n",
        "        pass  # If decoding fails, use original content\n",
        "    \n",
        "    json_objects = []\n",
        "    lines = content.split('\\n')\n",
        "    current_object = \"\"\n",
        "    brace_count = 0\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "            \n",
        "        current_object += line + \" \"\n",
        "        brace_count += line.count('{') - line.count('}')\n",
        "        \n",
        "        if brace_count == 0 and current_object.strip():\n",
        "            try:\n",
        "                clean_json = re.sub(r'\\s+', ' ', current_object.strip())\n",
        "                obj = json.loads(clean_json)\n",
        "                \n",
        "                # Recursively decode Unicode in the parsed object\n",
        "                def decode_in_object(item):\n",
        "                    if isinstance(item, dict):\n",
        "                        return {k: decode_in_object(v) for k, v in item.items()}\n",
        "                    elif isinstance(item, list):\n",
        "                        return [decode_in_object(i) for i in item]\n",
        "                    elif isinstance(item, str):\n",
        "                        # Replace common Unicode escapes\n",
        "                        return item.replace('\\\\u2026', 'â€¦').replace('\\\\u2019', \"'\").replace('\\\\u201c', '\"').replace('\\\\u201d', '\"')\n",
        "                    return item\n",
        "                \n",
        "                obj = decode_in_object(obj)\n",
        "                json_objects.append(obj)\n",
        "                \n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Failed to parse: {current_object[:100]}...\")\n",
        "                print(f\"Error: {e}\")\n",
        "            current_object = \"\"\n",
        "    \n",
        "    # Write with Unicode characters preserved\n",
        "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
        "        for obj in json_objects:\n",
        "            f.write(json.dumps(obj, separators=(',', ':'), ensure_ascii=False) + \"\\n\")\n",
        "    \n",
        "    return len(json_objects)\n",
        "\n",
        "# Fix with Unicode decoding\n",
        "count = fix_jsonl_with_unicode_decoding(\"yoda_chat_01.jsonl\", \"yoda_chat_02.jsonl\")\n",
        "print(f\"Processed {count} objects with Unicode decoding\")\n",
        "\n",
        "# Check a few entries to see if Unicode is fixed\n",
        "with open(\"yoda_chat_final.jsonl\", \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 3:  # Check first 3 entries\n",
        "            obj = json.loads(line)\n",
        "            print(f\"Entry {i+1}:\", obj)\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Unicode escapes: set()\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "with open(\"yoda_chat_02.jsonl\", \"r\") as f:\n",
        "    content = f.read()\n",
        "# Find all Unicode escape sequences\n",
        "unicode_escapes = re.findall(r'\\\\u[0-9a-fA-F]{4}', content)\n",
        "unique_escapes = set(unicode_escapes)\n",
        "print(\"Found Unicode escapes:\", unique_escapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 404 entries\n",
            "File size check: 404 entries loaded\n",
            "\n",
            "Sample entry:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Impossible! The Sith have been extinct for a millennium.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"The very Republic is threatened, if involved the Sith are.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Test loading your final file\n",
        "file = []\n",
        "with open(\"yoda_chat_02.jsonl\", \"r\") as f:\n",
        "    for line_num, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            file.append(json.loads(line))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error on line {line_num}: {e}\")\n",
        "            break\n",
        "\n",
        "print(f\"Successfully loaded {len(file)} entries\")\n",
        "print(f\"File size check: {len(file)} entries loaded\")\n",
        "\n",
        "# Show a sample entry to verify content looks good\n",
        "if file:\n",
        "    print(\"\\nSample entry:\")\n",
        "    print(json.dumps(file[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL3byIHfWEVx"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOTWElUCWk_v"
      },
      "outputs": [],
      "source": [
        "# For GPU check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x953lw83WxnY"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/mistral-7b-instruct-v0.3\"\n",
        "\n",
        "max_seq_length = 2048  # Choose sequence length\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIdADxFWXToO"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def format_prompt(example):\n",
        "    messages = {m[\"role\"]: m[\"content\"] for m in example[\"messages\"]}\n",
        "    user = messages.get(\"user\", \"\").strip()\n",
        "    yoda = messages.get(\"assistant\", \"\").strip()\n",
        "    return f\"[INST] {user} [/INST]\\n{yoda}\"\n",
        "\n",
        "# Apply to all examples and wrap in dict\n",
        "dataset = [{\"text\": format_prompt(ex)} for ex in file]\n",
        "\n",
        "dataset = Dataset.from_list(dataset)\n",
        "\n",
        "print(dataset[0])  # see the first formatted sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08de3wAXdu6"
      },
      "outputs": [],
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Rank stabilized LoRA\n",
        "    loftq_config=None, # LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "lm8booC8XliQ",
        "outputId": "63c698ae-0049-4d7e-f395-1a72773d0d59"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-504193230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training arguments optimized for Unsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer = SFTTrainer(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments optimized for Unsloth\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=\"none\", # Disable Weights & Biases logging\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZrtr0c4XmTE"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7vBGnx7DXuN9",
        "outputId": "6a83a84d-a0e8-48c3-f0e0-ba7dd60d5c2f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FastLanguageModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3771831152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enable native 2x faster inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Test prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m messages = [\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Test the fine-tuned model\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Test prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the Force\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "twEmkIrLZLtD",
        "outputId": "a44c7784-5ca9-4636-fb67-a0ed01f782e5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3751960265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Instead run this new code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained_merged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yoda_model_merged\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"merged_16bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpush_to_hub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# This model generates an error due to package conflicts. Package causing the error: entencepiece_model.proto\n",
        "# model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "# Load your trained weights\n",
        "# model.load_adapter(\"yoda_model\")  # This loads your adapter weights\n",
        "\n",
        "# Instead run this new code\n",
        "model.save_pretrained_merged(\"yoda_model_merged\", tokenizer, save_method=\"merged_16bit\", push_to_hub=False, max_shard_size=\"2GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eNME1Je5ttYC",
        "outputId": "c6f9f9c6-620a-41b0-8793-4257a18ff5c3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4187620778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FIRST: Save your model in HuggingFace format (this almost always works)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yoda_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yoda_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# THEN: Try the GGUF conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# FIRST: Save your model in HuggingFace format (this almost always works)\n",
        "model.save_pretrained(\"yoda_model\")\n",
        "tokenizer.save_pretrained(\"yoda_model\")\n",
        "\n",
        "# THEN: Try the GGUF conversion\n",
        "# If it fails, you still have your model saved!\n",
        "try:\n",
        "    model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "except Exception as e:\n",
        "    print(f\"GGUF conversion failed: {e}\")\n",
        "    print(\"But your model is safely saved in HuggingFace format!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQZeNhG-xBLz"
      },
      "outputs": [],
      "source": [
        "# This is to download the yoda model in hugging face format. I can even use this on my local machine later.\n",
        "\n",
        "# # Zip the folder first for easier download\n",
        "# import shutil\n",
        "# shutil.make_archive('yoda_model', 'zip', 'yoda_model')\n",
        "\n",
        "# # Then download the zip file\n",
        "# from google.colab import files\n",
        "# files.download('yoda_model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmERd43la0l"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "# if gguf_files:\n",
        "#     gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "#     print(f\"Downloading: {gguf_file}\")\n",
        "#     files.download(gguf_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
